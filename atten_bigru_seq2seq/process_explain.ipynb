{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intermedium/loaded_movie_lines.txt') as f:\n",
    "    read_in = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines: 221282\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\",\n",
       " \"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\",\n",
       " \"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\",\n",
       " \"You're asking me out.  That's so cute. What's your name again?\\tForget it.\",\n",
       " \"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Total number of lines: {len(read_in)}')\n",
    "read_in[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formalize(s):\n",
    "    \"\"\"\n",
    "    Formalize the given string by removing all non-alphabet characters\n",
    "\n",
    "    Args:\n",
    "        s(<str>): the given string\n",
    "    Return:\n",
    "        (<str>): A processed string\n",
    "    \"\"\"\n",
    "    # Turn a Unicode string to plain ASCII\n",
    "    temp_str = (s.encode('ascii', 'ignore')).decode('utf-8')\n",
    "\n",
    "    # Trim and remove all non-letter characters using regular expression\n",
    "    temp_str = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    temp_str = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)  \n",
    "\n",
    "    # Lowercase for final return\n",
    "    return temp_str.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each line and formalize each part to build the conversation pairs for later processing\n",
    "pairs = [[formalize(s) for s in l.split('\\t')] for l in read_in]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad. again.',\n",
       "  'well i thought we d start with pronunciation if that s okay with you.'],\n",
       " ['well i thought we d start with pronunciation if that s okay with you.',\n",
       "  'not the hacking and gagging and spitting part. please.'],\n",
       " ['not the hacking and gagging and spitting part. please.',\n",
       "  'okay... then how bout we try out some french cuisine. saturday? night?'],\n",
       " ['you re asking me out. that s so cute. what s your name again?',\n",
       "  'forget it.'],\n",
       " ['no no it s my fault we didn t have a proper introduction', 'cameron.']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(pairs, max_length=20):\n",
    "    \"\"\"\n",
    "    Filter pairs that under max_length threshold\n",
    "\n",
    "    Args:\n",
    "        pairs (list<list<str>>): The given list of pairs\n",
    "        max_length (int): The threshold for filtering pairs, default in 10\n",
    "    Return:\n",
    "        (list<list<str>>): The processed list of pairs that both length of sentences are under given max_length\n",
    "    \"\"\"\n",
    "    return [p for p in pairs \n",
    "            if len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 160465\n"
     ]
    }
   ],
   "source": [
    "filtered_pairs = filter(pairs, max_length=20)\n",
    "print(f'After filtering: {len(filtered_pairs)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "SOS = 1\n",
    "EOS = 2\n",
    "\n",
    "class IndexMapping:\n",
    "    \"\"\"\n",
    "    Map each unique word that encounter in the pairs to an index value\n",
    "    Then represent and store the discrete space by a dictionary\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        # self.name = name\n",
    "        self.word2index = {} # encode the word into an integer\n",
    "        self.index2word = {PAD: '<P>', SOS: '<S>', EOS: '<E>'} # decode the integer into a word\n",
    "        self.word2count = {} # count the occurence time of words\n",
    "        self.n_words = 3 # Count the SOS and EOS, then accumulate when new words come\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        # add the word into the dictionary and record its occurence time\n",
    "        if word not in self.word2index:\n",
    "            # If the word is new, then add it in the dictionary and count its number as 1\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word \n",
    "            self.word2count[word] = 1            \n",
    "            self.n_words += 1                    \n",
    "        else:\n",
    "            # If the word existed, just change its count number\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_map = IndexMapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words: 70811\n"
     ]
    }
   ],
   "source": [
    "for p in filtered_pairs:\n",
    "    num_map.add_sentence(p[0])\n",
    "    num_map.add_sentence(p[1])\n",
    "\n",
    "print(f'Counted words: {num_map.n_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_mapping(mapping, min_count):\n",
    "    \"\"\"\n",
    "    Trim the infrequently seen words in the given mapping \n",
    "    decided by the given minimum counts\n",
    "\n",
    "    Args:\n",
    "        mapping (dict): The given dict for triming\n",
    "        min_count (int): The threshold for the minimum count for triming, \n",
    "                         the word count in the original dict below the min_count will be removed\n",
    "    \"\"\"\n",
    "    keep_words = [] # Store all remaining words\n",
    "\n",
    "    for k, v in mapping.word2count.items():\n",
    "        if v > min_count:\n",
    "            keep_words.append(k) # Remove all words that the count is less than the threshold \n",
    "    \n",
    "    new_mapping = NumericalMapping() # Create a new mapping\n",
    "    \n",
    "    for w in keep_words:\n",
    "        new_mapping.add_word(w)\n",
    "    \n",
    "    return new_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_num_map = trim_mapping(num_map, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After triming, counted words: 29545\n"
     ]
    }
   ],
   "source": [
    "print(f'After triming, counted words: {trim_num_map.n_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_pairs(pairs, mapping):\n",
    "    \"\"\" \n",
    "    Trim the infrequently seen words in the given list of pairs \n",
    "    decided by the given mapping\n",
    "    \n",
    "    Args:\n",
    "        mapping (dict): the trim mapping that remove all infrequency seen words\n",
    "        pairs (list<list<str>>): the pairs for triming based on the mapping\n",
    "    Return:\n",
    "        (list<list<str>>): the trimed pairs\n",
    "    \"\"\"\n",
    "    keep_pairs = []\n",
    "\n",
    "    for pair in pairs:\n",
    "        keep_input, keep_output = True, True # Set flag for checking \n",
    "\n",
    "        # Check for the input sentence\n",
    "        for word in pair[0].split(' '):\n",
    "            if word not in mapping.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        \n",
    "        # Check for the output sentence\n",
    "        for word in pair[1].split(' '):\n",
    "            if word not in mapping.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "        \n",
    "        # Only keep the pair if the input and output sentence pass both checking\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "        \n",
    "    # for pair in pairs:\n",
    "    #     is_keep = True\n",
    "\n",
    "    #     for index in range(2):\n",
    "    #         for word in pair[index].split(' '):\n",
    "    #             if word not in mapping.word2index:\n",
    "    #                 is_keep = False\n",
    "    #                 break\n",
    "\n",
    "    #     if is_keep:\n",
    "    #         keep_pairs.append(pairs)     \n",
    "    \n",
    "    return keep_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pairs = trim_pairs(filtered_pairs, trim_num_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117117"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(selected_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e76b06f811d914f25ddf1d876c9e6424e54248baadb52cf54ff8d72e027625bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
